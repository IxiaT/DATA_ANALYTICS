{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34c46e71-b301-4b08-8469-aecaa0cd61cc",
   "metadata": {},
   "source": [
    "Instructions:\n",
    "-\n",
    "\n",
    "1. Read the article: https://www.sciencedirect.com/science/article/abs/pii/S0031320322001753\n",
    "2. Replicate the study using the same dataset.\n",
    "3. Read articles about Adjusted Rand Index, Normalized Mutual Information, and Folkes-Mallows Index (only use paper published in IEEE, sciencedirect, springerlink, Taylor Francis).\n",
    "4. Aside from the Adjusted Rand Index (ARI), and Normalized Mutual Information (NMI), use the Folkes-Mallows Index (FMI), and compare the result of each performance index.\n",
    "5. Compare and contrast each performance index, what are the advantages and disadvantages of ARI, NMI, and FMI, and when to use each?\n",
    "6. Using Kmodes and Hierarchical Clustering, use the same dataset and perform categorical data clustering, use FMI, ARI, and NMI for the comparison of performance.\n",
    "7. Write your report using Latex. Your report should be focused on the \"why's and the what's\" of each performance metrices (i.e. why is FMI always greater than ARI and NMI? What's the problem with ARI and NMI?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04abe27f-d9e6-48a2-8216-f680dbcd5ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import itertools\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import requests\n",
    "from itertools import combinations\n",
    "from io import StringIO\n",
    "from kmodes.kmodes import KModes\n",
    "from scipy.cluster.hierarchy import fcluster, linkage\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.manifold import SpectralEmbedding, TSNE\n",
    "from sklearn.metrics import adjusted_rand_score as ARI, normalized_mutual_info_score as NMI, fowlkes_mallows_score as FMI\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from tensorflow.keras import layers, Sequential\n",
    "import warnings\n",
    "\n",
    "# Adjust the Pandas display options for better readability\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 300)\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# List of datasets and their corresponding URLs\n",
    "dataset_urls = {\n",
    "    \"Soybean (Small)\": \"https://archive.ics.uci.edu/ml/machine-learning-databases/soybean/soybean-small.data\",\n",
    "    \"Zoo\": \"https://archive.ics.uci.edu/ml/machine-learning-databases/zoo/zoo.data\",\n",
    "    \"Heart Disease\": \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\",\n",
    "    \"Breast Cancer Wisconsin (Original)\": \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data\",\n",
    "    \"Dermatology\": \"https://archive.ics.uci.edu/ml/machine-learning-databases/dermatology/dermatology.data\",\n",
    "    \"Letter Recognition (E, F)\": \"https://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data\",\n",
    "    \"Molecular Biology (Splice-junction Gene Sequences)\": \"https://archive.ics.uci.edu/ml/machine-learning-databases/molecular-biology/splice-junction-gene-sequences/splice.data\",\n",
    "    \"Mushroom\": \"https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data\",\n",
    "    \"Iris\": \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\",\n",
    "    \"ISOLET (5)\": \"https://mikaelvincent.dev/datasets/isolet/isolet5.data\",\n",
    "    \"Optical Recognition of Handwritten Digits\": \"https://mikaelvincent.dev/datasets/opticaldigits/optdigits.data\",\n",
    "    \"Pen-Based Recognition of Handwritten Digits\": \"https://mikaelvincent.dev/datasets/pendigits/pendigits.data\"\n",
    "}\n",
    "# COIL20 not implemented; I don't understand how images can be translated into categories; I can't find a pre-processed version either :)\n",
    "\n",
    "# Set the true number of clusters for each dataset\n",
    "n_clusters_dict = {\n",
    "    \"Soybean (Small)\": 4,\n",
    "    \"Zoo\": 7,\n",
    "    \"Heart Disease\": 2,\n",
    "    \"Breast Cancer Wisconsin (Original)\": 2,\n",
    "    \"Dermatology\": 6,\n",
    "    \"Letter Recognition (E, F)\": 2,\n",
    "    \"Molecular Biology (Splice-junction Gene Sequences)\": 3,\n",
    "    \"Mushroom\": 2,\n",
    "    \"Iris\": 3,\n",
    "    \"ISOLET (5)\": 26,\n",
    "    \"Optical Recognition of Handwritten Digits\": 10,\n",
    "    \"Pen-Based Recognition of Handwritten Digits\": 10\n",
    "}\n",
    "\n",
    "# List of datasets for clustering ensemble task\n",
    "datasets_for_ensemble = [\n",
    "    \"Iris\",\n",
    "    \"ISOLET (5)\",\n",
    "    \"Optical Recognition of Handwritten Digits\",\n",
    "    \"Pen-Based Recognition of Handwritten Digits\"\n",
    "]\n",
    "\n",
    "# Set the number of runs for benchmarking\n",
    "num_runs = 10\n",
    "\n",
    "n_runs = 60  # Number of k-means runs for clustering ensemble\n",
    "\n",
    "dataframes = {}  # Dictionary to store cleaned dataframes\n",
    "\n",
    "for name, url in dataset_urls.items():\n",
    "    response = requests.get(url, verify=False)\n",
    "    data = response.text\n",
    "    \n",
    "    # Convert the CSV/Text data into a DataFrame\n",
    "    data_io = StringIO(data)\n",
    "    df = pd.read_csv(data_io, header=None)\n",
    "\n",
    "    # Set targets and features\n",
    "    if name == \"Letter Recognition (E, F)\":\n",
    "        y = df.iloc[:, 0]\n",
    "        X = df.iloc[:, 1:]\n",
    "    elif name == \"Mushroom\":\n",
    "        y = df.iloc[:, 0]\n",
    "        X = df.iloc[:, 1:]\n",
    "    elif name == \"Molecular Biology (Splice-junction Gene Sequences)\":\n",
    "        y = df.iloc[:, 0].str.strip()\n",
    "        X = pd.DataFrame([list(seq.strip()) for seq in df.iloc[:, 2]])\n",
    "    else:\n",
    "        X, y = df.iloc[:, :-1], df.iloc[:, -1]\n",
    "\n",
    "    # Drop columns with only 1 unique value\n",
    "    for col in X.columns:\n",
    "        if len(X[col].unique()) <= 1:\n",
    "            X.drop(columns=[col], inplace=True) # Diregard warning as it is behaving as expected\n",
    "    \n",
    "    # Store in the dataframes dictionary\n",
    "    dataframes[name] = {'features': X, 'targets': y}\n",
    "\n",
    "def preprocess_datasets(dataframes):\n",
    "    if 'Zoo' in dataframes:\n",
    "        zoo_df = dataframes['Zoo']['features']\n",
    "        zoo_df = zoo_df.drop(columns=[0])\n",
    "        dataframes['Zoo']['features'] = zoo_df\n",
    "\n",
    "    if 'Heart Disease' in dataframes:\n",
    "        hd_df = dataframes['Heart Disease']['features']\n",
    "        columns_to_drop = [0, 3, 4, 7, 9]\n",
    "        hd_df = hd_df.drop(columns=hd_df.columns[columns_to_drop])\n",
    "        dataframes['Heart Disease']['features'] = hd_df\n",
    "        y_hd = dataframes['Heart Disease']['targets']\n",
    "        dataframes['Heart Disease']['targets'] = y_hd.apply(lambda x: 0 if x == 0 else 1)\n",
    "    \n",
    "    if 'Breast Cancer Wisconsin (Original)' in dataframes:\n",
    "        bcw_df = dataframes['Breast Cancer Wisconsin (Original)']['features']\n",
    "        bcw_df = bcw_df.drop(columns=bcw_df.columns[0])\n",
    "        dataframes['Breast Cancer Wisconsin (Original)']['features'] = bcw_df\n",
    "    \n",
    "    if 'Dermatology' in dataframes:\n",
    "        derm_df = dataframes['Dermatology']['features']\n",
    "        derm_df = derm_df.drop(columns=derm_df.columns[-1])\n",
    "        dataframes['Dermatology']['features'] = derm_df\n",
    "\n",
    "    if 'Letter Recognition (E, F)' in dataframes:\n",
    "        lr_ef_df = dataframes['Letter Recognition (E, F)']['features']\n",
    "        lr_ef_targets = dataframes['Letter Recognition (E, F)']['targets']\n",
    "        mask = lr_ef_targets.isin(['E', 'F'])\n",
    "        dataframes['Letter Recognition (E, F)']['features'] = lr_ef_df[mask]\n",
    "        dataframes['Letter Recognition (E, F)']['targets'] = lr_ef_targets[mask]\n",
    "\n",
    "    return dataframes\n",
    "\n",
    "# Apply preprocessing to datasets\n",
    "dataframes = preprocess_datasets(dataframes)\n",
    "\n",
    "def run_multiple_kmeans(features, n_clusters, n_runs):\n",
    "    all_labels = []\n",
    "    for i in range(n_runs):\n",
    "        random_state = random.randint(0, 1000)\n",
    "        kmeans = KMeans(n_clusters=n_clusters, init='k-means++', random_state=random_state, n_init=10)\n",
    "        labels = kmeans.fit_predict(features)\n",
    "        all_labels.append(labels)\n",
    "    return np.array(all_labels).T\n",
    "\n",
    "for dataset_name in datasets_for_ensemble:\n",
    "    # Extracting features and targets from the preloaded datasets\n",
    "    features = dataframes[dataset_name][\"features\"]\n",
    "    targets = dataframes[dataset_name][\"targets\"]\n",
    "\n",
    "    # Converting targets to numerical labels if they aren't already\n",
    "    if targets.dtype.kind in 'O':  # Check if targets are object type (e.g., strings)\n",
    "        targets = LabelEncoder().fit_transform(targets)\n",
    "\n",
    "    # Determine the number of clusters from the unique elements in targets\n",
    "    n_clusters = len(np.unique(targets))\n",
    "\n",
    "    # Run multiple k-means and collect results\n",
    "    ensembled_features = run_multiple_kmeans(features, n_clusters, n_runs)\n",
    "    \n",
    "    # Convert numpy array to DataFrame and replace the original data\n",
    "    ensembled_features_df = pd.DataFrame(ensembled_features, index=features.index)\n",
    "    dataframes[dataset_name][\"features\"] = ensembled_features_df\n",
    "\n",
    "def perform_kmodes(features, n_clusters):\n",
    "    \"\"\"Perform clustering using KModes algorithm.\"\"\"\n",
    "    km = KModes(n_clusters=n_clusters, init='random', n_init=5)\n",
    "    clusters = km.fit_predict(features)\n",
    "    return clusters\n",
    "\n",
    "def perform_ordinal_encoding(features, true_labels, n_clusters):\n",
    "    \"\"\"Perform Ordinal Encoding followed by clustering.\"\"\"\n",
    "    encoder = LabelEncoder()\n",
    "    features_encoded = features.apply(encoder.fit_transform)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=10)\n",
    "    return kmeans.fit_predict(features_encoded, n_clusters)\n",
    "\n",
    "def perform_one_hot_encoding(features, true_labels, n_clusters):\n",
    "    \"\"\"Perform One-Hot Encoding followed by clustering.\"\"\"\n",
    "    encoder = OneHotEncoder()\n",
    "    features_encoded = encoder.fit_transform(features).toarray()\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=10)\n",
    "    return kmeans.fit_predict(features_encoded, n_clusters)\n",
    "\n",
    "def perform_link(features, n_clusters):\n",
    "    encoder = OneHotEncoder()\n",
    "    features_encoded = encoder.fit_transform(features).toarray()\n",
    "    # Calculate pairwise dissimilarities (1 - similarity), ensuring non-negative distances\n",
    "    ochiai_distance = pdist(features_encoded, lambda u, v: max(0, 1 - ochiai_coefficient_for_link(u, v)))\n",
    "    link_matrix = linkage(ochiai_distance, method='average')\n",
    "    clusters = fcluster(link_matrix, t=n_clusters, criterion='maxclust')\n",
    "    return clusters\n",
    "\n",
    "def ochiai_coefficient_for_link(b1, b2):\n",
    "    intersection = np.dot(b1, b2)\n",
    "    norm_b1 = np.sqrt(np.dot(b1, b1))\n",
    "    norm_b2 = np.sqrt(np.dot(b2, b2))\n",
    "    denominator = (norm_b1 * norm_b2)\n",
    "    if denominator == 0:\n",
    "        return 0  # Return 0 if either or both vectors are all zeros\n",
    "    return intersection / denominator\n",
    "\n",
    "def perform_cde(features, n_clusters):\n",
    "    \"\"\"Perform Categorical Data Embedding and clustering using t-SNE and k-Means.\"\"\"\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    features_encoded = encoder.fit_transform(features)\n",
    "    tsne_model = TSNE(n_components=2, perplexity=30, learning_rate=200)\n",
    "    features_embedded = tsne_model.fit_transform(features_encoded)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=10)\n",
    "    clusters = kmeans.fit_predict(features_embedded)\n",
    "    return clusters\n",
    "\n",
    "def perform_hierarchical_clustering(features, n_clusters, method='ward'):\n",
    "    \"\"\"Perform Hierarchical Clustering with handling for '?' characters and categorical variables.\"\"\"\n",
    "    \n",
    "    # Handle '?' characters and replace them appropriately\n",
    "    for column in features.columns:\n",
    "        if features[column].dtype == object:\n",
    "            # Check if the column contains '?'\n",
    "            if '?' in features[column].unique():\n",
    "                if features[column].str.isnumeric().any():\n",
    "                    # Assume numeric column with missing values represented as '?'\n",
    "                    # Convert '?' to NaN and then fill with the mean of the column\n",
    "                    features[column] = pd.to_numeric(features[column], errors='coerce')\n",
    "                    features[column].fillna(features[column].mean(), inplace=True)\n",
    "                else:\n",
    "                    # Categorical column\n",
    "                    mode_value = features[column].mode()[0]\n",
    "                    features[column] = features[column].replace('?', mode_value)\n",
    "            # Encode categorical variables\n",
    "            le = LabelEncoder()\n",
    "            features[column] = le.fit_transform(features[column].astype(str))\n",
    "        else:\n",
    "            # Directly fill missing values with mean if any\n",
    "            if features[column].isnull().any():\n",
    "                features[column].fillna(features[column].mean(), inplace=True)\n",
    "\n",
    "    # Ensure all data is in float format to avoid conversion errors in linkage\n",
    "    features = features.astype(float)\n",
    "\n",
    "    # Create the linkage matrix\n",
    "    Z = linkage(features, method=method)\n",
    "\n",
    "    # Create clusters by cutting the dendrogram at the specified number of clusters\n",
    "    clusters = fcluster(Z, t=n_clusters, criterion='maxclust')\n",
    "    return clusters\n",
    "\n",
    "def perform_cdc_dr(features, n_clusters, embedding_method='SE', operation='Joint'):\n",
    "    \"\"\"\n",
    "    Perform CDC_DR algorithm with specified graph embedding method and operation.\n",
    "    :param features: DataFrame of features\n",
    "    :param n_clusters: Number of clusters to form\n",
    "    :param embedding_method: 'NE', 'SE', 'NMF', or 'AE'\n",
    "    :param operation: 'Joint' or 'Mean'\n",
    "    :return: clusters - Cluster labels for each sample\n",
    "    \"\"\"\n",
    "    \n",
    "    # Construct similarity graph from features\n",
    "    graph = construct_similarity_graph(features)\n",
    "\n",
    "    # Apply graph embedding technique\n",
    "    embedded_graph = graph_embedding(graph, method=embedding_method)\n",
    "\n",
    "    value_to_index = create_value_to_index_mapping(features)\n",
    "    \n",
    "    # Ensure integrated_data is 2D before clustering\n",
    "    if operation == 'Joint':\n",
    "        integrated_data = joint_operation(embedded_graph, features, value_to_index)\n",
    "    elif operation == 'Mean':\n",
    "        integrated_data = mean_operation(embedded_graph, features, value_to_index)\n",
    "    else:\n",
    "        raise ValueError(\"Operation must be either 'Joint' or 'Mean'.\")\n",
    "    \n",
    "    # Impute missing values in integrated_data\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    integrated_data_imputed = imputer.fit_transform(integrated_data)\n",
    "    \n",
    "    # Cluster the integrated data\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=10)\n",
    "    clusters = kmeans.fit_predict(integrated_data_imputed)\n",
    "\n",
    "    return clusters\n",
    "\n",
    "def construct_similarity_graph(features):\n",
    "    \"\"\"\n",
    "    Construct a similarity graph from features based on categorical values.\n",
    "    :param features: DataFrame of features, each row is a sample and columns are categorical features\n",
    "    :return: graph - A NetworkX graph with nodes representing categorical values and weighted edges\n",
    "    \"\"\"\n",
    "    # Step 1: Prepare all unique categorical values and their indices\n",
    "    unique_values_dict = {}\n",
    "    for column in features:\n",
    "        unique_values = np.unique(features[column])\n",
    "        for val in unique_values:\n",
    "            unique_values_dict[f\"{column}_{val}\"] = np.where(features[column] == val)[0]\n",
    "    \n",
    "    # Step 2: Calculate similarity between all pairs of unique categorical values\n",
    "    graph = nx.Graph()\n",
    "    for (val1, indices1), (val2, indices2) in combinations(unique_values_dict.items(), 2):\n",
    "        # Calculate similarity (e.g., using Ochiai coefficient)\n",
    "        sim = ochiai_coefficient(indices1, indices2)  # Define this function based on your chosen similarity metric\n",
    "        if sim > 0:  # If the similarity is non-zero, add an edge\n",
    "            graph.add_edge(val1, val2, weight=sim)\n",
    "    \n",
    "    # Add all nodes explicitly in case some have no edges\n",
    "    for val in unique_values_dict.keys():\n",
    "        if val not in graph:\n",
    "            graph.add_node(val)\n",
    "\n",
    "    return graph\n",
    "\n",
    "def ochiai_coefficient(indices1, indices2):\n",
    "    \"\"\"\n",
    "    Calculate Ochiai coefficient between two sets of indices\n",
    "    :param indices1: array-like list of indices for the first categorical value\n",
    "    :param indices2: array-like list of indices for the second categorical value\n",
    "    :return: Ochiai coefficient as float\n",
    "    \"\"\"\n",
    "    set1 = set(indices1)\n",
    "    set2 = set(indices2)\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    if intersection == 0: return 0  # No overlap\n",
    "    return intersection / np.sqrt(len(set1) * len(set2))  # Ochiai coefficient formula\n",
    "\n",
    "def graph_embedding(graph, method='SE', dimensions=2):\n",
    "    \"\"\"\n",
    "    Apply graph embedding method to the constructed graph.\n",
    "    :param graph: NetworkX graph\n",
    "    :param method: string representing the graph embedding method: 'NE', 'SE', 'NMF', 'AE'\n",
    "    :param dimensions: the number of dimensions for the embedding\n",
    "    :return: embedded_graph - An array-like embedded representation of the graph\n",
    "    \"\"\"\n",
    "    # Convert graph to adjacency matrix and then to numpy ndarray\n",
    "    adjacency_matrix = nx.to_numpy_matrix(graph)\n",
    "    adjacency_matrix = np.asarray(adjacency_matrix)\n",
    "    \n",
    "    if method == 'NE':\n",
    "        # Directly use the adjacency matrix as features (no embedding)\n",
    "        embedded_graph = adjacency_matrix\n",
    "\n",
    "    elif method == 'SE':\n",
    "        # Apply Spectral Embedding\n",
    "        embedding_model = SpectralEmbedding(n_components=dimensions)\n",
    "        embedded_graph = embedding_model.fit_transform(adjacency_matrix)\n",
    "\n",
    "    elif method == 'NMF':\n",
    "        # Apply Non-negative Matrix Factorization for embedding\n",
    "        model = NMF(n_components=dimensions, init='random', max_iter=10000)\n",
    "        embedded_graph = model.fit_transform(adjacency_matrix)\n",
    "\n",
    "    elif method == 'AE':\n",
    "        # Apply Autoencoder for graph embedding\n",
    "        n_nodes = adjacency_matrix.shape[0]\n",
    "        input_dim = n_nodes\n",
    "        autoencoder = Sequential([\n",
    "            layers.Dense(64, activation='relu', input_shape=(input_dim,)),\n",
    "            layers.Dense(dimensions, activation='relu'),  # Embedding layer\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(input_dim, activation='sigmoid')\n",
    "        ])\n",
    "        autoencoder.compile(optimizer='adam', loss='mse')\n",
    "        adjacency_matrix_norm = adjacency_matrix / np.max(adjacency_matrix)  # Normalize adjacency matrix\n",
    "        autoencoder.fit(adjacency_matrix_norm, adjacency_matrix_norm, epochs=50, verbose=0)\n",
    "        encoder = Sequential(autoencoder.layers[:2])  # The first two layers are the encoder\n",
    "        embedded_graph = encoder.predict(adjacency_matrix_norm, verbose=0)\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Graph embedding method {method} is not implemented.\")\n",
    "    \n",
    "    return np.array(embedded_graph)\n",
    "\n",
    "def create_value_to_index_mapping(features):\n",
    "    \"\"\"\n",
    "    Create a mapping from each unique categorical value to a unique index.\n",
    "    :param features: DataFrame of features, each column is a categorical feature\n",
    "    :return: Dictionary of value to index mapping\n",
    "    \"\"\"\n",
    "    # Extracting unique values from each feature\n",
    "    unique_values = set()\n",
    "    for column in features.columns:\n",
    "        unique_values.update(features[column].unique())\n",
    "\n",
    "    # Creating a mapping from unique values to an index\n",
    "    value_to_index = {value: idx for idx, value in enumerate(unique_values)}\n",
    "    return value_to_index\n",
    "\n",
    "def joint_operation(embedded_graph, features, value_to_index):\n",
    "    # Concatenates the embeddings for each categorical value in each sample\n",
    "    joint_embedded = []\n",
    "    for _, row in features.iterrows():\n",
    "        joint_vector = []\n",
    "        for value in row:\n",
    "            index = value_to_index[value]  # Map each categorical value to its index in the embedded graph\n",
    "            joint_vector.extend(embedded_graph[index])\n",
    "        joint_embedded.append(joint_vector)\n",
    "    return np.array(joint_embedded)\n",
    "\n",
    "def mean_operation(embedded_graph, features, value_to_index):\n",
    "    # Calculates the mean of the embeddings for each categorical value in each sample\n",
    "    mean_embedded = []\n",
    "    for _, row in features.iterrows():\n",
    "        vectors = [embedded_graph[value_to_index[value]] for value in row]\n",
    "        mean_vector = np.mean(vectors, axis=0)\n",
    "        mean_embedded.append(mean_vector)\n",
    "    return np.array(mean_embedded)\n",
    "\n",
    "\n",
    "def run_clustering_algorithms(dataframes, n_clusters_dict, num_runs=10):\n",
    "    results_list = []\n",
    "    for name, data in dataframes.items():\n",
    "        print(\"Processing:\", name)\n",
    "        features = data['features']\n",
    "        true_labels = data['targets'].squeeze()  # Assuming targets are in a single column\n",
    "        n_clusters = n_clusters_dict.get(name, 2)  # Default to 2 clusters if not specified\n",
    "\n",
    "        metrics = {'KModes': [], 'Ordinal': [], 'One-Hot': [], 'Link': [], 'CDE': [], 'Hierarchical': []}  # Initialize a dictionary to store results for each method\n",
    "\n",
    "        # Include CDC_DR methods in metrics dictionary\n",
    "        embedding_methods = ['NE', 'SE', 'NMF', 'AE']  # Non-Embedding, Spectral Embedding, Nonnegative Matrix Factorization, Autoencoder\n",
    "        operations = ['Joint', 'Mean']  # The two types of operations\n",
    "        \n",
    "        for em in embedding_methods:\n",
    "            for op in operations:\n",
    "                key_name = f\"CDC_DR+{em} ({op})\"\n",
    "                metrics[key_name] = []\n",
    "\n",
    "        for _ in range(num_runs):\n",
    "            # KModes\n",
    "            km_clusters = perform_kmodes(features, n_clusters)\n",
    "            ari, nmi, fmi = calculate_metrics(true_labels, km_clusters)\n",
    "            metrics['KModes'].append((ari, nmi, fmi))\n",
    "\n",
    "            # Ordinal Encoding\n",
    "            ord_clusters = perform_ordinal_encoding(features, true_labels, n_clusters)\n",
    "            ari, nmi, fmi = calculate_metrics(true_labels, ord_clusters)\n",
    "            metrics['Ordinal'].append((ari, nmi, fmi))\n",
    "\n",
    "            # One-Hot Encoding\n",
    "            oh_clusters = perform_one_hot_encoding(features, true_labels, n_clusters)\n",
    "            ari, nmi, fmi = calculate_metrics(true_labels, oh_clusters)\n",
    "            metrics['One-Hot'].append((ari, nmi, fmi))\n",
    "\n",
    "            # Link with Ochiai Coefficient\n",
    "            link_clusters = perform_link(features, n_clusters)\n",
    "            ari, nmi, fmi = calculate_metrics(true_labels, link_clusters)\n",
    "            metrics['Link'].append((ari, nmi, fmi))\n",
    "\n",
    "            # CDE with t-SNE and k-Means\n",
    "            cde_clusters = perform_cde(features, n_clusters)\n",
    "            ari, nmi, fmi = calculate_metrics(true_labels, cde_clusters)\n",
    "            metrics['CDE'].append((ari, nmi, fmi))\n",
    "\n",
    "            # Hierarchical Clustering\n",
    "            hier_clusters = perform_hierarchical_clustering(features, n_clusters)\n",
    "            ari, nmi, fmi = ARI(true_labels, hier_clusters), NMI(true_labels, hier_clusters), FMI(true_labels, hier_clusters)\n",
    "            metrics['Hierarchical'].append((ari, nmi, fmi))\n",
    "\n",
    "            # CDC_DR with various embedding methods and operations\n",
    "            for embedding_method in ['NE', 'SE', 'NMF', 'AE']:\n",
    "                for operation in ['Joint', 'Mean']:\n",
    "                    cdc_dr_clusters = perform_cdc_dr(features, n_clusters, embedding_method, operation)\n",
    "                    ari, nmi, fmi = calculate_metrics(true_labels, cdc_dr_clusters)\n",
    "                    metrics[f\"CDC_DR+{embedding_method} ({operation})\"].append((ari, nmi, fmi))\n",
    "\n",
    "        # Calculate mean and standard deviation for each method and append to results list\n",
    "        for method, values in metrics.items():\n",
    "            ari_vals, nmi_vals, fmi_vals = zip(*values)\n",
    "            ari_mean, ari_std = np.mean(ari_vals), np.std(ari_vals)\n",
    "            nmi_mean, nmi_std = np.mean(nmi_vals), np.std(nmi_vals)\n",
    "            fmi_mean, fmi_std = np.mean(fmi_vals), np.std(fmi_vals)\n",
    "            results_list.append({\n",
    "                \"Dataset\": name,\n",
    "                \"Method\": method,\n",
    "                \"ARI\": f\"{ari_mean:.4f}±{ari_std:.2f}\",\n",
    "                \"NMI\": f\"{nmi_mean:.4f}±{nmi_std:.2f}\",\n",
    "                \"FMI\": f\"{fmi_mean:.4f}±{fmi_std:.2f}\"\n",
    "            })\n",
    "\n",
    "    # Convert list of dictionaries to DataFrame for results\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "    return results_df\n",
    "\n",
    "def calculate_metrics(true_labels, predicted_labels):\n",
    "    \"\"\"Calculate clustering metrics: Adjusted Rand Index, Normalized Mutual Information, and Folkes-Mallows Index.\"\"\"\n",
    "    return ARI(true_labels, predicted_labels), NMI(true_labels, predicted_labels), FMI(true_labels, predicted_labels)\n",
    "\n",
    "def reformat_results(results_df):\n",
    "    # Expanding 'ARI', 'NMI', and 'FMI' columns into multiple rows with a new 'Metric' column\n",
    "    expanded_df = pd.melt(results_df, id_vars=[\"Dataset\", \"Method\"], value_vars=[\"ARI\", \"NMI\", \"FMI\"], var_name=\"Metric\", value_name=\"Value\")\n",
    "    expanded_df[['Metric_Value', 'Std']] = expanded_df['Value'].str.split('±', expand=True)\n",
    "    expanded_df.drop(columns=['Value'], inplace=True)  # Removing the original combined column\n",
    "    \n",
    "    # Convert the 'Metric_Value' and 'Std' columns to numeric types\n",
    "    expanded_df['Metric_Value'] = expanded_df['Metric_Value'].astype(float)\n",
    "    expanded_df['Std'] = expanded_df['Std'].astype(float)\n",
    "\n",
    "    # Concatenate the metric value and standard deviation back into a single string\n",
    "    expanded_df['Metric_Value'] = expanded_df['Metric_Value'].map('{:.4f}'.format) + \"±\" + expanded_df['Std'].map('{:.2f}'.format)\n",
    "    \n",
    "    # Ensuring the order of datasets and methods remains consistent with the original DataFrame\n",
    "    dataset_order = results_df['Dataset'].unique()\n",
    "    method_order = results_df['Method'].unique()\n",
    "\n",
    "    # Creating a pivot table to restructure the DataFrame as required\n",
    "    pivot_df = expanded_df.pivot_table(index=[\"Dataset\", \"Metric\"], columns=\"Method\", values=\"Metric_Value\", aggfunc='first')\n",
    "    \n",
    "    # Reindexing the pivot table to maintain the original order\n",
    "    pivot_df = pivot_df.reindex(dataset_order, level='Dataset')\n",
    "    pivot_df = pivot_df.reindex(method_order, axis='columns')\n",
    "\n",
    "    return pivot_df\n",
    "\n",
    "# Running all algorithms and storing the results\n",
    "results = run_clustering_algorithms(dataframes, n_clusters_dict, num_runs)\n",
    "\n",
    "Processing: Soybean (Small)\n",
    "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001723BAC0790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
    "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001723BAC0CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
    "Processing: Zoo\n",
    "Processing: Heart Disease\n",
    "Processing: Breast Cancer Wisconsin (Original)\n",
    "Processing: Dermatology\n",
    "Processing: Letter Recognition (E, F)\n",
    "Processing: Molecular Biology (Splice-junction Gene Sequences)\n",
    "Processing: Mushroom\n",
    "Processing: Iris\n",
    "Processing: ISOLET (5)\n",
    "Processing: Optical Recognition of Handwritten Digits\n",
    "Processing: Pen-Based Recognition of Handwritten Digits\n",
    "\n",
    "# Use the function to reformat the results\n",
    "formatted_results = reformat_results(results)\n",
    "\n",
    "Section: Presentation of Results\n",
    "\n",
    "# Prepare data for the table\n",
    "table_data = []\n",
    "\n",
    "for name, content in dataframes.items():\n",
    "    X, y = content['features'], content['targets']\n",
    "    table_data.append({\n",
    "        \"Name\": name,\n",
    "        \"Number of Samples\": X.shape[0],\n",
    "        \"Number of Features\": X.shape[1],\n",
    "        \"Number of Unique Values in Target\": len(pd.unique(y))\n",
    "    })\n",
    "\n",
    "# Convert table data into a DataFrame for pretty printing\n",
    "table_df = pd.DataFrame(table_data)\n",
    "print(table_df)\n",
    "\n",
    "print(\"\\n* Number of features for numerical datasets are based on the # of runs of kmeans in the clustering ensemble. For an accurate measure of the original versions of the datasets, run this before running the clustering ensemble.\")\n",
    "\n",
    "                                                 Name  Number of Samples  Number of Features  Number of Unique Values in Target\n",
    "0                                     Soybean (Small)                 47                  21                                  4\n",
    "1                                                 Zoo                101                  16                                  7\n",
    "2                                       Heart Disease                303                   8                                  2\n",
    "3                  Breast Cancer Wisconsin (Original)                699                   9                                  2\n",
    "4                                         Dermatology                366                  33                                  6\n",
    "5                           Letter Recognition (E, F)               1543                  16                                  2\n",
    "6   Molecular Biology (Splice-junction Gene Sequen...               3190                  60                                  3\n",
    "7                                            Mushroom               8124                  21                                  2\n",
    "8                                                Iris                150                  60                                  3\n",
    "9                                          ISOLET (5)               1559                  60                                 26\n",
    "10          Optical Recognition of Handwritten Digits               5620                  60                                 10\n",
    "11        Pen-Based Recognition of Handwritten Digits              10992                  60                                 10\n",
    "\n",
    "* Number of features for numerical datasets are based on the # of runs of kmeans in the clustering ensemble. For an accurate measure of the original versions of the datasets, run this before running the clustering ensemble.\n",
    "\n",
    "# Print the formatted results\n",
    "print(formatted_results)\n",
    "\n",
    "Method                                                          KModes      Ordinal      One-Hot          Link          CDE Hierarchical CDC_DR+NE (Joint) CDC_DR+NE (Mean) CDC_DR+SE (Joint) CDC_DR+SE (Mean) CDC_DR+NMF (Joint) CDC_DR+NMF (Mean) CDC_DR+AE (Joint) CDC_DR+AE (Mean)\n",
    "Dataset                                            Metric                                                                                                                                                                                                                             \n",
    "Soybean (Small)                                    ARI     0.9097±0.14  0.5458±0.00  1.0000±0.00   1.0000±0.00  0.9420±0.08  1.0000±0.00       1.0000±0.00      0.0120±0.01       0.5562±0.00      0.1079±0.00        1.0000±0.00      -0.0046±0.00       0.7776±0.32      0.0013±0.03\n",
    "                                                   FMI     0.9319±0.11  0.6573±0.00  1.0000±0.00   1.0000±0.00  0.9565±0.06  1.0000±0.00       1.0000±0.00      0.2768±0.00       0.6692±0.00      0.3795±0.00        1.0000±0.00       0.2788±0.00       0.8598±0.18      0.3101±0.10\n",
    "                                                   NMI     0.9422±0.09  0.7160±0.00  1.0000±0.00   1.0000±0.00  0.9540±0.06  1.0000±0.00       1.0000±0.00      0.1369±0.01       0.7248±0.00      0.2554±0.00        1.0000±0.00       0.0843±0.00       0.8351±0.29      0.0746±0.05\n",
    "Zoo                                                ARI     0.6810±0.09  0.7315±0.12  0.7156±0.09   0.8893±0.00  0.6483±0.04  0.6958±0.00       0.7455±0.09      0.3029±0.04       0.8175±0.07      0.1677±0.01        0.7086±0.07       0.1502±0.01       0.5810±0.20      0.1682±0.11\n",
    "                                                   FMI     0.7519±0.07  0.7916±0.09  0.7813±0.07   0.9162±0.00  0.7291±0.03  0.7641±0.00       0.8041±0.07      0.4422±0.03       0.8597±0.05      0.3353±0.01        0.7741±0.06       0.3242±0.00       0.7011±0.09      0.3924±0.07\n",
    "                                                   NMI     0.7955±0.03  0.8023±0.04  0.8410±0.03   0.8731±0.00  0.8262±0.02  0.7935±0.00       0.8393±0.03      0.4744±0.04       0.8665±0.02      0.3751±0.01        0.8113±0.03       0.3607±0.00       0.7222±0.24      0.3213±0.17\n",
    "Heart Disease                                      ARI     0.3733±0.02  0.4515±0.01  0.4234±0.01   0.2915±0.10  0.3513±0.03  0.3707±0.00       0.4381±0.00      0.2661±0.00       0.1412±0.00      0.0146±0.00        0.2661±0.00       0.1402±0.03       0.2567±0.15      0.1072±0.07\n",
    "                                                   FMI     0.6924±0.01  0.7284±0.01  0.7119±0.00   0.6680±0.01  0.6766±0.01  0.6856±0.00       0.7238±0.00      0.6331±0.00       0.5715±0.00      0.5407±0.00        0.6335±0.00       0.5707±0.01       0.6737±0.04      0.5779±0.05\n",
    "                                                   NMI     0.2942±0.02  0.3566±0.01  0.3346±0.00   0.2227±0.07  0.2721±0.02  0.2889±0.00       0.3482±0.00      0.2049±0.00       0.1117±0.00      0.0250±0.00        0.2025±0.00       0.1048±0.02       0.1998±0.12      0.0882±0.05\n",
    "Breast Cancer Wisconsin (Original)                 ARI     0.7104±0.05  0.8173±0.00  0.7983±0.00   0.0026±0.00  0.7898±0.06  0.8502±0.00       0.7579±0.00      0.7087±0.00       0.6118±0.00      0.6065±0.00        0.7172±0.00       0.6650±0.01       0.6547±0.02      0.2849±0.14\n",
    "                                                   FMI     0.8752±0.02  0.9183±0.00  0.9064±0.00   0.7397±0.00  0.9027±0.03  0.9314±0.00       0.8871±0.00      0.8638±0.00       0.8167±0.00      0.8150±0.00        0.8685±0.00       0.8441±0.00       0.8374±0.01      0.6617±0.07\n",
    "                                                   NMI     0.6064±0.04  0.7100±0.00  0.7249±0.00   0.0047±0.00  0.7021±0.06  0.7606±0.00       0.6883±0.00      0.6297±0.00       0.5542±0.00      0.5239±0.00        0.6200±0.00       0.5606±0.01       0.5956±0.02      0.2293±0.13\n",
    "Dermatology                                        ARI     0.5092±0.06  0.7321±0.06  0.8516±0.11   0.5468±0.00  0.7136±0.02  0.7121±0.00       0.6873±0.05      0.1862±0.00       0.6269±0.06      0.1528±0.00        0.7086±0.06       0.1799±0.00       0.6703±0.05      0.1403±0.07\n",
    "                                                   FMI     0.6069±0.04  0.7852±0.05  0.8811±0.09   0.6971±0.00  0.7686±0.02  0.7691±0.00       0.7480±0.05      0.3408±0.00       0.6990±0.05      0.3121±0.00        0.7662±0.05       0.3370±0.00       0.7354±0.04      0.3541±0.05\n",
    "                                                   NMI     0.6136±0.05  0.8805±0.01  0.9074±0.03   0.7161±0.00  0.8100±0.01  0.8763±0.00       0.7850±0.04      0.3180±0.00       0.6514±0.04      0.2761±0.01        0.7952±0.03       0.3109±0.00       0.7574±0.05      0.2413±0.12\n",
    "Letter Recognition (E, F)                          ARI     0.1992±0.07  0.5200±0.00  0.2564±0.01   0.2108±0.00  0.6563±0.01  0.0500±0.00       0.6006±0.01      0.2130±0.00       0.2000±0.00      0.1594±0.00        0.1997±0.00       0.1141±0.01       0.2055±0.07      0.2072±0.07\n",
    "                                                   FMI     0.6097±0.04  0.7647±0.00  0.6296±0.01   0.6594±0.00  0.8280±0.00  0.5356±0.00       0.8037±0.00      0.6064±0.00       0.6006±0.00      0.5801±0.00        0.6004±0.00       0.5569±0.00       0.6040±0.03      0.6255±0.03\n",
    "                                                   NMI     0.1663±0.06  0.4632±0.00  0.1975±0.01   0.3098±0.00  0.5478±0.01  0.0397±0.00       0.5509±0.00      0.1603±0.00       0.1512±0.00      0.1194±0.00        0.1509±0.00       0.0844±0.00       0.1565±0.05      0.1587±0.05\n",
    "Molecular Biology (Splice-junction Gene Sequences) ARI     0.0266±0.01  0.0418±0.03  0.5781±0.03  -0.0074±0.00  0.1913±0.04  0.0637±0.00       0.6148±0.02      0.0576±0.00       0.3813±0.00      0.0281±0.00        0.2772±0.01       0.0527±0.00       0.3393±0.07      0.0450±0.01\n",
    "                                                   FMI     0.3770±0.01  0.4127±0.00  0.7335±0.02   0.6108±0.00  0.4820±0.03  0.4213±0.00       0.7564±0.01      0.3958±0.00       0.6050±0.00      0.3943±0.00        0.5411±0.00       0.3928±0.00       0.5775±0.05      0.3987±0.00\n",
    "                                                   NMI     0.0414±0.02  0.0626±0.03  0.5319±0.02   0.0119±0.00  0.2321±0.05  0.0552±0.00       0.5669±0.02      0.0680±0.00       0.3693±0.00      0.0475±0.00        0.2481±0.01       0.0608±0.00       0.3425±0.07      0.0584±0.01\n",
    "Mushroom                                           ARI     0.6138±0.00  0.1729±0.00  0.6186±0.00   0.0009±0.00  0.3642±0.12  0.1695±0.00       0.6147±0.00      0.0324±0.00       0.3205±0.00      0.0179±0.00        0.4783±0.11       0.0407±0.00       0.2523±0.08      0.0216±0.01\n",
    "                                                   FMI     0.8123±0.00  0.6409±0.00  0.8143±0.00   0.7040±0.00  0.6855±0.06  0.6377±0.00       0.8123±0.00      0.5175±0.00       0.6604±0.00      0.5124±0.00        0.7440±0.05       0.5214±0.00       0.6654±0.02      0.5116±0.00\n",
    "                                                   NMI     0.5697±0.01  0.1935±0.00  0.5692±0.00   0.0109±0.00  0.3056±0.10  0.1854±0.00       0.5620±0.00      0.0232±0.00       0.2459±0.00      0.0141±0.00        0.4102±0.11       0.0292±0.00       0.2565±0.05      0.0155±0.00\n",
    "Iris                                               ARI     0.7302±0.00  0.7302±0.00  0.7302±0.00   0.7302±0.00  0.7302±0.00  0.7302±0.00       0.7302±0.00      0.7302±0.00       0.7302±0.00      0.7302±0.00        0.3651±0.37       0.1460±0.29       0.0000±0.00      0.3651±0.37\n",
    "                                                   FMI     0.8208±0.00  0.8208±0.00  0.8208±0.00   0.8208±0.00  0.8208±0.00  0.8208±0.00       0.8208±0.00      0.8208±0.00       0.8208±0.00      0.8208±0.00        0.6971±0.12       0.6229±0.10       0.5735±0.00      0.6971±0.12\n",
    "                                                   NMI     0.7582±0.00  0.7582±0.00  0.7582±0.00   0.7582±0.00  0.7582±0.00  0.7582±0.00       0.7582±0.00      0.7582±0.00       0.7582±0.00      0.7582±0.00        0.3791±0.38       0.1516±0.30       0.0000±0.00      0.3791±0.38\n",
    "ISOLET (5)                                         ARI     0.4385±0.02  0.4635±0.01  0.4669±0.00   0.4737±0.00  0.4631±0.01  0.4534±0.00       0.4728±0.01      0.3353±0.01       0.4349±0.01      0.2076±0.01        0.4614±0.01       0.2092±0.01       0.4534±0.02      0.2210±0.02\n",
    "                                                   FMI     0.4646±0.02  0.4859±0.01  0.4890±0.00   0.4983±0.00  0.4849±0.01  0.4767±0.00       0.4945±0.01      0.3632±0.01       0.4585±0.01      0.2438±0.01        0.4835±0.01       0.2440±0.01       0.4761±0.02      0.2546±0.02\n",
    "                                                   NMI     0.7053±0.01  0.7224±0.00  0.7258±0.00   0.7270±0.00  0.7237±0.01  0.7213±0.00       0.7258±0.00      0.5974±0.01       0.7053±0.01      0.4663±0.00        0.7217±0.00       0.4651±0.00       0.7166±0.01      0.4836±0.02\n",
    "Optical Recognition of Handwritten Digits          ARI     0.6704±0.00  0.6704±0.00  0.6704±0.00   0.6704±0.00  0.6837±0.00  0.6707±0.00       0.6704±0.00      0.6685±0.00       0.6737±0.01      0.6152±0.03        0.6706±0.00       0.5188±0.04       0.6705±0.00      0.6272±0.04\n",
    "                                                   FMI     0.7063±0.00  0.7063±0.00  0.7063±0.00   0.7063±0.00  0.7164±0.00  0.7066±0.00       0.7063±0.00      0.7046±0.00       0.7090±0.01      0.6594±0.03        0.7065±0.00       0.5860±0.03       0.7064±0.00      0.6696±0.03\n",
    "                                                   NMI     0.7566±0.00  0.7562±0.00  0.7566±0.00   0.7566±0.00  0.7503±0.00  0.7570±0.00       0.7566±0.00      0.7523±0.00       0.7569±0.00      0.7079±0.02        0.7565±0.00       0.6582±0.02       0.7565±0.00      0.7208±0.02\n",
    "Pen-Based Recognition of Handwritten Digits        ARI     0.5820±0.03  0.5648±0.00  0.5319±0.00   0.5319±0.00  0.5533±0.00  0.5638±0.00       0.5320±0.00      0.4933±0.00       0.5541±0.03      0.4751±0.03        0.5612±0.01       0.4969±0.02       0.5498±0.03      0.4819±0.02\n",
    "                                                   FMI     0.6273±0.02  0.6108±0.00  0.5874±0.00   0.5874±0.00  0.6005±0.00  0.6098±0.00       0.5874±0.00      0.5569±0.00       0.6056±0.03      0.5415±0.02        0.6086±0.01       0.5561±0.02       0.6016±0.02      0.5457±0.02\n",
    "                                                   NMI     0.6914±0.01  0.6794±0.00  0.6820±0.00   0.6820±0.00  0.6678±0.00  0.6788±0.00       0.6821±0.00      0.6432±0.00       0.6892±0.01      0.6175±0.02        0.6831±0.00       0.6210±0.02       0.6869±0.01      0.6185±0.02\n",
    "\n",
    "Questions\n",
    "\n",
    "The Adjusted Rand Index (ARI), Normalized Mutual Information (NMI), and Fowlkes-Mallows Index (FMI) are all metrics used to evaluate the performance of clustering algorithms by comparing the clustering results with ground truth labels.\n",
    "Adjusted Rand Index (ARI)\n",
    "\n",
    "    Advantages: Corrects for chance grouping, making it more reliable in indicating actual similarities between clusters and ground truth. Values range from -1 to 1, where 1 indicates perfect agreement, and values less than 0 indicate independent labelings.\n",
    "    Disadvantages: Can be more computationally intensive than simpler metrics; less intuitive to interpret without statistical background.\n",
    "    Use Case: Best for benchmarking different clustering algorithms when a ground truth is available.\n",
    "\n",
    "Normalized Mutual Information (NMI)\n",
    "\n",
    "    Advantages: Also adjusts for chance and is normalized between 0 and 1, where 1 indicates a perfect match between clusters and ground truth. It measures mutual information in a way that accounts for the sizes of different clusters.\n",
    "    Disadvantages: Its normalization can sometimes lead to misleading results, especially if the number of clusters is very different from the number of ground truth categories.\n",
    "    Use Case: Useful in cases where clusters are of varying sizes and a normalized measure is preferred.\n",
    "\n",
    "Fowlkes-Mallows Index (FMI)\n",
    "\n",
    "    Advantages: Based on the geometric mean of precision and recall, providing a balance between them. Simple to calculate and understand, with values ranging from 0 to 1, where 1 indicates perfect clustering.\n",
    "    Disadvantages: Does not adjust for chance, which can make it overly optimistic in scenarios where random clustering might appear effective.\n",
    "    Use Case: Effective for quick assessments where computational simplicity is needed and the number of clusters is close to the number of categories in ground truth.\n",
    "\n",
    "When to Use Each\n",
    "\n",
    "    ARI is preferred when an accurate and chance-corrected measure is critical.\n",
    "    NMI is suitable for comparing clusters of varying sizes and when normalization of mutual information is beneficial.\n",
    "    FMI is good for quick, intuitive assessments of clustering when the expected number of clusters is not significantly different from the number of ground truth categories.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
